{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import torch\n",
    "from transformers import AutoModelForCTC, Wav2Vec2Processor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "from src.text_processing import *\n",
    "from src.data_processing import *\n",
    "from src.ui_tools import *\n",
    "from src.audio_processing import *\n",
    "\n",
    "# Add espeak's shared library directory\n",
    "os.environ['DYLD_LIBRARY_PATH'] = '/opt/homebrew/lib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned data\n",
    "data_path = 'data/df_test_cleaned.csv'\n",
    "data_cleaned = pd.read_csv(data_path)\n",
    "\n",
    "# Apply conversion functions to testResults and evaluationResults columns\n",
    "data_cleaned['testResults'] = data_cleaned['testResults'].apply(lambda x: convert_str_to_dct_eval(x))\n",
    "data_cleaned['evaluationResults'] = data_cleaned['evaluationResults'].apply(lambda x: convert_str_to_dct_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tests id of the 10 selected readingTestFluencE tests\n",
    "tests_id = [\n",
    "    '2BB671AA-2F6A-4346-8B76-F0C89C236390',\n",
    "    '3B545E56-D802-4380-9993-21C11066B12E',\n",
    "    '5C1C826F-E778-48C3-9170-6BF943175984',\n",
    "    '046E4FEB-E284-48D5-922E-616DA7651F02',\n",
    "    '75A80925-F8CF-463D-AFED-5CC399848CC2',\n",
    "    '102DCD09-43EA-434D-A590-0FA5C7C7C1B3',\n",
    "    '098522E8-2203-425E-85E5-5809D5B0B523',\n",
    "    '79055215-1979-42D3-9B26-B9C6DD935D83',\n",
    "    'ABD81BE7-7629-4816-8241-7ECBF32DFFFA',\n",
    "    'DC79B554-B33E-4E01-83BC-3B97798C5F97'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phonetic transcription saved to transcriptions/readingTestFluencE_transcriptions.csv\n"
     ]
    }
   ],
   "source": [
    "# We extract the text that is read by the children during the readingTestFluencE tests\n",
    "test = data_cleaned[data_cleaned['id'] == tests_id[0]]['testParameters'].values[0]\n",
    "test_dict = ast.literal_eval(test)\n",
    "selected_text = test_dict['textSelected']['text']\n",
    "\n",
    "save_phonetic_transcription_to_csv(selected_text, test_type='readingTestFluencE', folder='transcriptions', file_name='readingTestFluencE_transcriptions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only keep the rows where the id is in the list of tests_id\n",
    "readingTests = data_cleaned[data_cleaned['id'].apply(lambda x: x in tests_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd1cc7d7eaf46eab8cb333021253809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Previous', style=ButtonStyle()), IntText(value=0, description='Index:'), Buâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b4559f9c424da393ed4e6a477e03d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52f306b083a4c609e3417c7fd366554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract recordings and their corresponding evaluation results (e.g., 'wordsState')\n",
    "recordings = readingTests['testResults'].apply(\n",
    "    lambda x: x['recording'] if 'recording' in x else None).dropna().tolist()\n",
    "\n",
    "evaluation_results = readingTests['evaluationResults'].apply(\n",
    "    lambda x: x['wordsState'] if 'wordsState' in x else None).dropna().tolist()\n",
    "\n",
    "# Create the interactive audio player with evaluation results\n",
    "create_audio_player_with_results(recordings, evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and processor\n",
    "MODEL_ID = \"Cnam-LMSSC/wav2vec2-french-phonemizer\"\n",
    "model = AutoModelForCTC.from_pretrained(MODEL_ID)\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing the audio files\n",
    "audio_folder = 'converted_wav_files/'\n",
    "\n",
    "# List all the .wav files in the folder\n",
    "audio_files = [f for f in os.listdir(audio_folder) if f.endswith('.wav')]\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "transcriptions = []\n",
    "\n",
    "# Process each file\n",
    "for audio_file in audio_files:\n",
    "    # Load the audio file\n",
    "    audio_path = os.path.join(audio_folder, audio_file)\n",
    "    audio, _ = sf.read(audio_path)\n",
    "    \n",
    "    # Preprocess the audio and prepare the inputs for the model\n",
    "    inputs = processor(np.array(audio), sampling_rate=16_000., return_tensors=\"pt\")\n",
    "    \n",
    "    # Get the model's predictions\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Decode the predictions to get the phonetic transcription\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "    \n",
    "    # Store the result (file name and transcription)\n",
    "    transcriptions.append([audio_file, transcription])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results to a DataFrame and save them to a CSV file\n",
    "transcriptions_df = pd.DataFrame(transcriptions, columns=['File Name', 'Phonetic Transcription'])\n",
    "transcriptions_df.to_csv('transcriptions/readingTestFluencE_children.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

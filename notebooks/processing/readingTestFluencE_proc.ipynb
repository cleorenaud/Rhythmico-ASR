{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import torch\n",
    "from transformers import AutoModelForCTC, Wav2Vec2Processor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "from src.text_processing import *\n",
    "from src.data_processing import *\n",
    "from src.ui_tools import *\n",
    "from src.audio_processing import *\n",
    "\n",
    "# Add espeak's shared library directory\n",
    "os.environ['DYLD_LIBRARY_PATH'] = '/opt/homebrew/lib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kn/xrv1vh8j0k99dnh2mwl662xc0000gn/T/ipykernel_10543/537599615.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  readingTestFluencE_df['testResults'] = readingTestFluencE_df['testResults'].apply(lambda x: convert_str_to_dct_eval(x))\n",
      "/var/folders/kn/xrv1vh8j0k99dnh2mwl662xc0000gn/T/ipykernel_10543/537599615.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  readingTestFluencE_df['evaluationResults'] = readingTestFluencE_df['evaluationResults'].apply(lambda x: convert_str_to_dct_eval(x))\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned data\n",
    "data_path = 'data/df_test_cleaned.csv'\n",
    "tests_df = pd.read_csv(data_path)\n",
    "\n",
    "# We only keep the rows where the testType is readingTestFluencE\n",
    "readingTestFluencE_df = tests_df[tests_df['testType'] == 'readingTestFluencE']\n",
    "\n",
    "# Apply conversion functions to testResults and evaluationResults columns\n",
    "readingTestFluencE_df['testResults'] = readingTestFluencE_df['testResults'].apply(lambda x: convert_str_to_dct_eval(x))\n",
    "readingTestFluencE_df['evaluationResults'] = readingTestFluencE_df['evaluationResults'].apply(lambda x: convert_str_to_dct_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We extract the audio files and convert them to .wav format\n",
    "save_recordings_as_wav(readingTestFluencE_df, output_dir='wav_files/readingTestFluencE', target_sample_rate=16000, channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tests id of the 10 selected readingTestFluencE tests\n",
    "tests_id = [\n",
    "    '2BB671AA-2F6A-4346-8B76-F0C89C236390',\n",
    "    '3B545E56-D802-4380-9993-21C11066B12E',\n",
    "    '5C1C826F-E778-48C3-9170-6BF943175984',\n",
    "    '046E4FEB-E284-48D5-922E-616DA7651F02',\n",
    "    '75A80925-F8CF-463D-AFED-5CC399848CC2',\n",
    "    '102DCD09-43EA-434D-A590-0FA5C7C7C1B3',\n",
    "    '098522E8-2203-425E-85E5-5809D5B0B523',\n",
    "    '79055215-1979-42D3-9B26-B9C6DD935D83',\n",
    "    'ABD81BE7-7629-4816-8241-7ECBF32DFFFA',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07483f720d24961a2d3dc13838ee0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Previous', style=ButtonStyle()), IntText(value=0, description='Index:'), Bu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8f083d2b464773bdaa204b56695c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe6253b6c7c4f4d91b36b8d41978796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We extract the row with id = 75A80925-F8CF-463D-AFED-5CC399848CC2\n",
    "test_id = '75A80925-F8CF-463D-AFED-5CC399848CC2'\n",
    "test_row = readingTestFluencE_df[readingTestFluencE_df['id'] == test_id]\n",
    "\n",
    "# Extract recordings and their corresponding evaluation results (e.g., 'wordsState')\n",
    "recording = test_row['testResults'].apply(\n",
    "    lambda x: x['recording'] if 'recording' in x else None).dropna().tolist()\n",
    "\n",
    "evaluation_result = test_row['evaluationResults'].apply(\n",
    "    lambda x: x['wordsState'] if 'wordsState' in x else None).dropna().tolist()\n",
    "\n",
    "# Create the interactive audio player with evaluation results\n",
    "create_audio_player_with_results(recording, evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phonetic transcription saved to transcriptions/readingTestFluencE_transcriptions.csv\n"
     ]
    }
   ],
   "source": [
    "# If the transcription file already exists, we do nothing\n",
    "transcription_file = 'testPhoneme_deletion_transcriptions.csv'\n",
    "file_exists = os.path.isfile(transcription_file)\n",
    "\n",
    "if not(file_exists):\n",
    "    # We extract the text that is read by the children during the readingTestFluencE tests\n",
    "    test = readingTestFluencE_df[readingTestFluencE_df['id'] == tests_id[0]]['testParameters'].values[0]\n",
    "    test_dict = ast.literal_eval(test)\n",
    "    selected_text = test_dict['textSelected']['text']\n",
    "\n",
    "    save_phonetic_transcription_to_csv(selected_text, test_type='readingTestFluencE', folder='transcriptions', file_name='readingTestFluencE_transcriptions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only keep the rows where the id is in the list of tests_id\n",
    "readingTests = readingTestFluencE_df[readingTestFluencE_df['id'].apply(lambda x: x in tests_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce6ca70bda543449a8d9317e9951cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Previous', style=ButtonStyle()), IntText(value=0, description='Index:'), Bu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8260c18ed2048599723ee04807c753b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1eeb4cf32dd422cb25a18941997e33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract recordings and their corresponding evaluation results (e.g., 'wordsState')\n",
    "recordings = readingTests['testResults'].apply(\n",
    "    lambda x: x['recording'] if 'recording' in x else None).dropna().tolist()\n",
    "\n",
    "evaluation_results = readingTests['evaluationResults'].apply(\n",
    "    lambda x: x['wordsState'] if 'wordsState' in x else None).dropna().tolist()\n",
    "\n",
    "# Create the interactive audio player with evaluation results\n",
    "create_audio_player_with_results(recordings, evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and processor\n",
    "MODEL_ID = \"Cnam-LMSSC/wav2vec2-french-phonemizer\"\n",
    "model = AutoModelForCTC.from_pretrained(MODEL_ID)\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing the audio files\n",
    "audio_folder = 'wav_files/readingTestFluencE/'\n",
    "\n",
    "# List all the .wav files in the folder\n",
    "audio_files = [f for f in os.listdir(audio_folder) if f.endswith('.wav')]\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "transcriptions = []\n",
    "\n",
    "# Process each file\n",
    "for audio_file in audio_files:\n",
    "    # Load the audio file\n",
    "    audio_path = os.path.join(audio_folder, audio_file)\n",
    "    audio, _ = sf.read(audio_path)\n",
    "    \n",
    "    # Preprocess the audio and prepare the inputs for the model\n",
    "    inputs = processor(np.array(audio), sampling_rate=16_000., return_tensors=\"pt\")\n",
    "    \n",
    "    # Get the model's predictions\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Decode the predictions to get the phonetic transcription\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "    \n",
    "    # Store the result (file name and transcription)\n",
    "    transcriptions.append([audio_file, transcription])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results to a DataFrame and save them to a CSV file\n",
    "transcriptions_df = pd.DataFrame(transcriptions, columns=['File Name', 'Phonetic Transcription'])\n",
    "transcriptions_df.to_csv('transcriptions/readingTestFluencE_children.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
